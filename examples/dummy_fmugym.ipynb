{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225772d6",
   "metadata": {},
   "source": [
    "# Dummy example for gym environment with FMU with HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11deb528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from fmpy import read_model_description, extract\n",
    "\n",
    "from fmugym import FMUGym, FMUGymConfig, VarSpace, State2Out, TargetValue\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC, HerReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d19b1",
   "metadata": {},
   "source": [
    "### 2 Implement abstract FMUGym class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca189da-c534-4e14-8498-06e98b21b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMUEnv(FMUGym):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    # Used by step() and reset(), returns any relevant debugging information.\n",
    "    def _get_info(self):\n",
    "        return {'info_time':time.time()}\n",
    "\n",
    "    # Retrieves FMU output values by possibly calling self.fmu_get_fmu_output for handling different FMU versions\n",
    "    # and stores them in the self.observation dictionary. \n",
    "    # It can also add output noise (using self._get_output_noise()) and update the set point \n",
    "    def _get_obs(self):\n",
    "        \n",
    "        self._get_fmu_output()\n",
    "\n",
    "        obs = np.array(list(self.observation.values())).flatten()\n",
    "        \n",
    "        noisy_observation = obs + self._get_output_noise()\n",
    "        \n",
    "        setpoint = self.setpoint_trajectory(self.y_start, self.y_stop, self.time)\n",
    "\n",
    "        obs_dict = OrderedDict([\n",
    "            ('observation', np.array(noisy_observation)),\n",
    "            ('achieved_goal', np.array(noisy_observation)),\n",
    "            ('desired_goal', setpoint.astype(np.float32))\n",
    "        ])\n",
    "        return obs_dict\n",
    "\n",
    "    # Returns input noise for each input component, potentially by sampling from the self.input_noise dictionary.\n",
    "    def _get_input_noise(self):\n",
    "        input_noise = []\n",
    "        for inp_name in self.input_dict:\n",
    "            input_noise.append(self.input_noise[inp_name].sample()[0])\n",
    "        return np.array(input_noise)\n",
    "\n",
    "    # Similar to self._get_input_noise, generates output noise for each output component,\n",
    "    # potentially by sampling from the self.output_noise dictionary.\n",
    "    def _get_output_noise(self):\n",
    "        output_noise = []\n",
    "        for out_name in self.output_dict:\n",
    "            output_noise.append(self.output_noise[out_name].sample()[0])\n",
    "        return np.array(output_noise)\n",
    "\n",
    "    # Returns two booleans indicating first the termination and second truncation status. \n",
    "    def _get_terminated(self):\n",
    "        if self.time > self.stop_time:\n",
    "                self.reset()\n",
    "                return True, False\n",
    "    \n",
    "        for termination in self.terminations:\n",
    "            min_value = self.terminations[termination].low[0]\n",
    "            max_value = self.terminations[termination].high[0]\n",
    "            if self.observation[termination] < min_value or self.observation[termination] > max_value:\n",
    "                self.reset()\n",
    "                print(\"truncated\")\n",
    "                return False, True\n",
    "                    \n",
    "        return False, False\n",
    "\n",
    "    # Constructs the action space from a VarSpace object representing the inputs. \n",
    "    # It can use gymnasium.spaces.Box for continuous action spaces.\n",
    "    def _create_action_space(self, inputs):\n",
    "        lows = []\n",
    "        highs = []\n",
    "        for inp in inputs:\n",
    "            lows.append(inputs[inp].low[0])\n",
    "            highs.append(inputs[inp].high[0])\n",
    "        action_space = gym.spaces.Box(low=np.array(lows), high=np.array(highs), dtype=np.float32)\n",
    "        return action_space\n",
    "\n",
    "    # Constructs the observation space returning it as a gymnasium.spaces.Dict. \n",
    "    # The observation space typically includes observation, achieved_goal, and desired_goal, each created from a VarSpace object.\n",
    "    def _create_observation_space(self, outputs):\n",
    "        lows = []\n",
    "        highs = []\n",
    "        for out in outputs:\n",
    "            lows.append(outputs[out].low[0])\n",
    "            highs.append(outputs[out].high[0])\n",
    "        observation_space = gym.spaces.Dict({\n",
    "            'observation': gym.spaces.Box(low=np.array(lows), high=np.array(highs), dtype=np.float32),\n",
    "            'achieved_goal': gym.spaces.Box(low=np.array(lows), high=np.array(highs), dtype=np.float32),\n",
    "            'desired_goal': gym.spaces.Box(low=np.array(lows), high=np.array(highs), dtype=np.float32)\n",
    "        })\n",
    "        return observation_space\n",
    "\n",
    "    # Random variations to initial system states and dynamic parameters by sampling from self.random_vars_refs \n",
    "    # and propagates to corresponding initial output values.  \n",
    "    # It also allows for direct manipulation and randomization of set point goals using the self.y_stop class variable.\n",
    "    def _noisy_init(self):\n",
    "        # add noise to setpoint goals\n",
    "        for ye in self.y_stop:\n",
    "            self.y_stop[ye] = self.y_stop_range[ye].sample()[0]\n",
    "        \n",
    "        # add noise to initial system state\n",
    "        init_states = {}\n",
    "        for var in self.random_vars_refs:\n",
    "            var_ref = self.random_vars_refs[var][0]\n",
    "            uniform_value = self.random_vars_refs[var][1].sample()[0]\n",
    "            init_states[var_ref] = uniform_value\n",
    "\n",
    "            # domain randomization with noisy initial y_start\n",
    "            if var in self.rand_starts.keys():\n",
    "                input_string = self.rand_starts[var]\n",
    "                self.y_start[input_string] = float(uniform_value)\n",
    "        \n",
    "        return init_states\n",
    "\n",
    "    # Called by self.step() to add noise to action from RL library.\n",
    "    # May be used to execute low-level controller and adapt action space.\n",
    "    def _process_action(self, action):\n",
    "        processed_action = action + self._get_input_noise()\n",
    "        return processed_action\n",
    "\n",
    "    # Determines the set point values at the current time step within the trajectory, called by self._get_obs().\n",
    "    def setpoint_trajectory(self, y_start, y_stop, time):\n",
    "        y = []\n",
    "        for y0, ye in zip(y_start.values(), y_stop.values()):\n",
    "            y.append((ye - y0)/(self.stop_time-self.start_time)*(time-self.start_time) + y0)\n",
    "        return np.array(y)\n",
    "\n",
    "    # Interface between step method of fmugym and compute_reward. adjusts the necessary paramters for reward method of RL library (SKRL, SB3).\n",
    "    def _process_reward(self, obs, acts, info):\n",
    "        achieved_goal = obs[\"achieved_goal\"]\n",
    "        desired_goal = obs[\"desired_goal\"]\n",
    "        reward = self.compute_reward(achieved_goal, desired_goal, info)\n",
    "        return reward\n",
    "\n",
    "    # Computes and returns a scalar reward value from achieved_goal, desired_goal, and possibly further parameters.\n",
    "    def compute_reward(self, achieved_goal, desired_goal, info):\n",
    "        \"\"\"\n",
    "        compute reward for HER\n",
    "            achieved_goal: outputs of FMU\n",
    "            desired_goal: current setpoint of trajectory\n",
    "            info: [NOT USED]\n",
    "        Returns:\n",
    "            float: environment reward\n",
    "        \"\"\"\n",
    "\n",
    "        # Deceptive reward: it is positive (0) only when the goal is achieved\n",
    "        # Here we are using a vectorized version\n",
    "        boundary_accuracy = 0.1\n",
    "        control_error = achieved_goal - desired_goal\n",
    "        if len(control_error) == self.observation_space[\"observation\"].shape[0]:\n",
    "                boundary_violated = False\n",
    "                for err in control_error:\n",
    "                    boundary_violated = boundary_violated or (abs(err)>boundary_accuracy)            \n",
    "                reward = -(boundary_violated.astype(np.float32))\n",
    "                \n",
    "        else:\n",
    "            reward = []\n",
    "            for ctrl_err in control_error:\n",
    "                boundary_violated = False\n",
    "                for err in ctrl_err:\n",
    "                    boundary_violated = boundary_violated or (abs(err)>boundary_accuracy)\n",
    "                reward.append(-(boundary_violated.astype(np.float32)))\n",
    "        \n",
    "        return np.array(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725804e5-8b5c-4b58-999a-b56efcf6acc1",
   "metadata": {},
   "source": [
    "### 3 Create FMUGymConfig object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e20963-cfc0-4e45-8d6e-f8df2f1c3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# providing inputs, outputs and their noises with range of values\n",
    "inputs = VarSpace(\"inputs\")\n",
    "inputs.add_var_box(\"input1\", -2.0, 2.0)\n",
    "inputs.add_var_box(\"input2\", -2.0, 2.0)\n",
    "\n",
    "input_noise = VarSpace(\"input_noise\")\n",
    "input_noise.add_var_box(\"input1\", 0.0, 0.0)\n",
    "input_noise.add_var_box(\"input2\", 0.0, 0.0)\n",
    "\n",
    "outputs = VarSpace(\"outputs\")\n",
    "outputs.add_var_box(\"output1\", -1e6, 1e6)\n",
    "outputs.add_var_box(\"output2\", -1e6, 1e6)\n",
    "\n",
    "output_noise = VarSpace(\"output_noise\")\n",
    "output_noise.add_var_box(\"output1\", 0.0, 0.0)\n",
    "output_noise.add_var_box(\"output2\", 0.0, 0.0)\n",
    "\n",
    "# randomized dynamics parameters and their range of values\n",
    "random_vars = VarSpace(\"random_vars\")\n",
    "random_vars.add_var_box(\"firstOrder.k\", 4, 6)\n",
    "random_vars.add_var_box(\"firstOrder1.k\", 4, 6)\n",
    "# randomized initial initial states and their range of values\n",
    "random_vars.add_var_box(\"firstOrder.y_start\", -0.5, 0.5)\n",
    "random_vars.add_var_box(\"firstOrder1.y_start\", -0.5, 0.5)\n",
    "\n",
    "# map state variables to corresponding outputs of Modelica Model\n",
    "set_point_map = State2Out(\"set_point_map\")\n",
    "set_point_map.add_map(\"firstOrder.y_start\", \"output1\")\n",
    "set_point_map.add_map(\"firstOrder1.y_start\", \"output2\")\n",
    "\n",
    "# set point values for the start and stop of the trajectory\n",
    "set_point_nominal_start = TargetValue(\"set_point_nominal_start\")\n",
    "set_point_nominal_start.add_target(\"output1\", 0.0)\n",
    "set_point_nominal_start.add_target(\"output2\", 0.0)\n",
    "\n",
    "set_point_stop = VarSpace(\"set_point_stop\")\n",
    "set_point_stop.add_var_box(\"output1\", 1.0, 2.5)\n",
    "set_point_stop.add_var_box(\"output2\", 1.2, 3.0)\n",
    "\n",
    "# allowed range of output values, if exceeded termination (or rather truncation) of the episode\n",
    "terminations = VarSpace(\"terminations\")\n",
    "terminations.add_var_box(\"output1\", -0.6, 3.0)\n",
    "terminations.add_var_box(\"output2\", -0.6, 3.5)\n",
    "\n",
    "# create FMUGymConfig object from all the above defined parameters\n",
    "config = FMUGymConfig(fmu_path=os.path.abspath('FMUs/dummy_for_FMU_cosim.fmu'),\n",
    "                      start_time=0.0,\n",
    "                      stop_time=10.0,\n",
    "                      sim_step_size=0.01,\n",
    "                      action_step_size=0.01,\n",
    "                      inputs=inputs,\n",
    "                      input_noise=input_noise,\n",
    "                      outputs=outputs,\n",
    "                      output_noise=output_noise,\n",
    "                      random_vars=random_vars,\n",
    "                      set_point_map=set_point_map,\n",
    "                      set_point_nominal_start=set_point_nominal_start,\n",
    "                      set_point_stop=set_point_stop,\n",
    "                      terminations=terminations\n",
    "                     )                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82741f5-2bd4-45dc-966e-3083f2cb13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_description = read_model_description(config.fmu_path)\n",
    "print(model_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe83cf",
   "metadata": {},
   "source": [
    "### 4 Creation of FMUEnv instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9baf88f-91cb-481b-9e96-0c6808ca36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummyEnv = FMUEnv(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143def4d",
   "metadata": {},
   "source": [
    "### 5 Creation of RL agent from FMUEnv and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f9181",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC(\n",
    "    \"MultiInputPolicy\",\n",
    "    dummyEnv,\n",
    "    replay_buffer_class=HerReplayBuffer,\n",
    "    replay_buffer_kwargs=dict(\n",
    "        n_sampled_goal=6,\n",
    "        goal_selection_strategy=\"future\",\n",
    "    ),\n",
    "    ent_coef=\"auto_1.0\",\n",
    "    learning_starts = int(2*(config.stop_time - config.start_time)/config.sim_step_size),\n",
    "    gamma=0.99,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timer_start_time = time.time()\n",
    "model.learn(total_timesteps=1e5)\n",
    "print(\"learn duration: \", time.time() - timer_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4852fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_agents/sac_agent_dummy_rand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a95826",
   "metadata": {},
   "source": [
    "### ... or loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = SAC.load(\"trained_agents/sac_agent_dummy_rand.zip\", dummyEnv, print_system_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615fb30",
   "metadata": {},
   "source": [
    "### 6 Evaluate performance of policy with inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c66bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the environment and setting random initial and target values\n",
    "observation, info = dummyEnv.reset()\n",
    "# capturing trajectories here\n",
    "obs_array = []\n",
    "act_array = []\n",
    "obs_array.append(observation[\"observation\"])\n",
    "\n",
    "num_steps = int((config.stop_time - config.start_time) / config.sim_step_size)\n",
    "for _ in range(num_steps):\n",
    "    action, _state = model.predict(observation, deterministic=True) \n",
    "    observation, reward, terminated, truncated, info = dummyEnv.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = dummyEnv.reset()\n",
    "    obs_array.append(observation[\"observation\"])\n",
    "    act_array.append(action)\n",
    "\n",
    "obs_array = np.array(obs_array).flatten()\n",
    "obs_array = obs_array.reshape(-1, 2)  \n",
    "act_array = np.array(act_array).flatten()\n",
    "act_array = act_array.reshape(-1, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42713211-7a10-4222-88cb-ec8db733f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing setpoint trajectory used at inference\n",
    "y1_setpoints = []\n",
    "y2_setpoints = []\n",
    "\n",
    "for t in np.arange(config.start_time, config.stop_time, config.sim_step_size):\n",
    "    y1_setpoints.append(dummyEnv.setpoint_trajectory(dummyEnv.y_start, dummyEnv.y_stop, t)[0])\n",
    "    y2_setpoints.append(dummyEnv.setpoint_trajectory(dummyEnv.y_start, dummyEnv.y_stop, t)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(obs_array[:, 0], label='y1', color=\"b\")\n",
    "plt.plot(y1_setpoints, label='y1_set', color=\"b\", linestyle='--')\n",
    "plt.plot(obs_array[:, 1], label='y2', color=\"r\")\n",
    "plt.plot(y2_setpoints, label='y2_set', color=\"r\", linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(act_array[:, 0], label='u1', color=\"b\")\n",
    "plt.plot(act_array[:, 1], label='u2', color=\"r\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4872f6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ba3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
