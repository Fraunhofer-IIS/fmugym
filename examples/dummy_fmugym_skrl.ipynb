{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225772d6",
   "metadata": {},
   "source": [
    "# Dummy example for gym environment with FMU and SKRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11deb528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from fmpy import read_model_description, extract\n",
    "\n",
    "from fmugym import FMUGym, FMUGymConfig, VarSpace, State2Out, TargetValue\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee57e2f-4259-40d0-b73a-323e9378f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import the skrl components to build the RL system\n",
    "from skrl.agents.torch.sac import SAC_DEFAULT_CONFIG\n",
    "from skrl.agents.torch.sac import SAC_RNN as SAC\n",
    "from skrl.envs.wrappers.torch import wrap_env\n",
    "from skrl.memories.torch import RandomMemory\n",
    "from skrl.models.torch import DeterministicMixin, GaussianMixin, Model\n",
    "from skrl.trainers.torch import SequentialTrainer\n",
    "from skrl.utils import set_seed\n",
    "from skrl.envs.wrappers.torch import wrap_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb1897f-f129-49ac-b885-6e85a93b9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6708c6ab-b54a-4ae3-b00f-aeb482f08e00",
   "metadata": {},
   "source": [
    "## Define model of NN for SAC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4729c2-3d1e-4095-9131-ceb1a3f62fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models (stochastic and deterministic models) using mixins\n",
    "class Actor(GaussianMixin, Model):\n",
    "    def __init__(self, observation_space, action_space, device, clip_actions=False,\n",
    "                 clip_log_std=True, min_log_std=-20, max_log_std=2, reduction=\"sum\",\n",
    "                 num_envs=1, num_layers=1, hidden_size=256, sequence_length=20):\n",
    "        Model.__init__(self, observation_space, action_space, device)\n",
    "        GaussianMixin.__init__(self, clip_actions, clip_log_std, min_log_std, max_log_std, reduction)\n",
    "\n",
    "        self.num_envs = num_envs\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size  # Hout\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=self.num_observations,\n",
    "                          hidden_size=self.hidden_size,\n",
    "                          num_layers=self.num_layers,\n",
    "                          batch_first=True)  # batch_first -> (batch, sequence, features)\n",
    "\n",
    "        self.linear_layer_1 = nn.Linear(self.hidden_size, 256)\n",
    "        self.action_layer = nn.Linear(256, self.num_actions)\n",
    "\n",
    "        self.log_std_parameter = nn.Parameter(torch.zeros(self.num_actions))\n",
    "\n",
    "    def get_specification(self):\n",
    "        # batch size (N) is the number of envs\n",
    "        return {\"rnn\": {\"sequence_length\": self.sequence_length,\n",
    "                        \"sizes\": [(self.num_layers, self.num_envs, self.hidden_size)]}}  # hidden states (D ∗ num_layers, N, Hout)\n",
    "\n",
    "    def compute(self, inputs, role):\n",
    "        states = inputs[\"states\"]\n",
    "        terminated = inputs.get(\"terminated\", None)\n",
    "        hidden_states = inputs[\"rnn\"][0]\n",
    "\n",
    "        # training\n",
    "        if self.training:\n",
    "            rnn_input = states.view(-1, self.sequence_length, states.shape[-1])  # (N, L, Hin): N=batch_size, L=sequence_length\n",
    "            hidden_states = hidden_states.view(self.num_layers, -1, self.sequence_length, hidden_states.shape[-1])  # (D * num_layers, N, L, Hout)\n",
    "            # get the hidden states corresponding to the initial sequence\n",
    "            hidden_states = hidden_states[:,:,0,:].contiguous()  # (D * num_layers, N, Hout)\n",
    "\n",
    "            # reset the RNN state in the middle of a sequence\n",
    "            if terminated is not None and torch.any(terminated):\n",
    "                rnn_outputs = []\n",
    "                terminated = terminated.view(-1, self.sequence_length)\n",
    "                indexes = [0] + (terminated[:,:-1].any(dim=0).nonzero(as_tuple=True)[0] + 1).tolist() + [self.sequence_length]\n",
    "\n",
    "                for i in range(len(indexes) - 1):\n",
    "                    i0, i1 = indexes[i], indexes[i + 1]\n",
    "                    rnn_output, hidden_states = self.rnn(rnn_input[:,i0:i1,:], hidden_states)\n",
    "                    hidden_states[:, (terminated[:,i1-1]), :] = 0\n",
    "                    rnn_outputs.append(rnn_output)\n",
    "\n",
    "                rnn_output = torch.cat(rnn_outputs, dim=1)\n",
    "            # no need to reset the RNN state in the sequence\n",
    "            else:\n",
    "                rnn_output, hidden_states = self.rnn(rnn_input, hidden_states)\n",
    "        # rollout\n",
    "        else:\n",
    "            rnn_input = states.view(-1, 1, states.shape[-1])  # (N, L, Hin): N=num_envs, L=1\n",
    "            rnn_output, hidden_states = self.rnn(rnn_input, hidden_states)\n",
    "\n",
    "        # flatten the RNN output\n",
    "        rnn_output = torch.flatten(rnn_output, start_dim=0, end_dim=1)  # (N, L, D ∗ Hout) -> (N * L, D ∗ Hout)\n",
    "\n",
    "        x = F.relu(self.linear_layer_1(rnn_output))\n",
    "\n",
    "        # Pendulum-v1 action_space is -2 to 2\n",
    "        return 2 * torch.tanh(self.action_layer(x)), self.log_std_parameter, {\"rnn\": [hidden_states]}\n",
    "\n",
    "class Critic(DeterministicMixin, Model):\n",
    "    def __init__(self, observation_space, action_space, device, clip_actions=False,\n",
    "                 num_envs=1, num_layers=1, hidden_size=256, sequence_length=20):\n",
    "        Model.__init__(self, observation_space, action_space, device)\n",
    "        DeterministicMixin.__init__(self, clip_actions)\n",
    "\n",
    "        self.num_envs = num_envs\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size  # Hout\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=self.num_observations,\n",
    "                          hidden_size=self.hidden_size,\n",
    "                          num_layers=self.num_layers,\n",
    "                          batch_first=True)  # batch_first -> (batch, sequence, features)\n",
    "\n",
    "        self.linear_layer_1 = nn.Linear(self.hidden_size + self.num_actions, 256)\n",
    "        self.linear_layer_3 = nn.Linear(256, 1)\n",
    "\n",
    "    def get_specification(self):\n",
    "        # batch size (N) is the number of envs\n",
    "        return {\"rnn\": {\"sequence_length\": self.sequence_length,\n",
    "                        \"sizes\": [(self.num_layers, self.num_envs, self.hidden_size)]}}  # hidden states (D ∗ num_layers, N, Hout)\n",
    "\n",
    "    def compute(self, inputs, role):\n",
    "        states = inputs[\"states\"]\n",
    "        terminated = inputs.get(\"terminated\", None)\n",
    "        hidden_states = inputs[\"rnn\"][0]\n",
    "\n",
    "        # critic is only used during training\n",
    "        rnn_input = states.view(-1, self.sequence_length, states.shape[-1])  # (N, L, Hin): N=batch_size, L=sequence_length\n",
    "\n",
    "        hidden_states = hidden_states.view(self.num_layers, -1, self.sequence_length, hidden_states.shape[-1])  # (D * num_layers, N, L, Hout)\n",
    "        # get the hidden states corresponding to the initial sequence\n",
    "        sequence_index = 1 if role in [\"target_critic_1\", \"target_critic_2\"] else 0  # target networks act on the next state of the environment\n",
    "        hidden_states = hidden_states[:,:,sequence_index,:].contiguous()  # (D * num_layers, N, Hout)\n",
    "\n",
    "        # reset the RNN state in the middle of a sequence\n",
    "        if terminated is not None and torch.any(terminated):\n",
    "            rnn_outputs = []\n",
    "            terminated = terminated.view(-1, self.sequence_length)\n",
    "            indexes = [0] + (terminated[:,:-1].any(dim=0).nonzero(as_tuple=True)[0] + 1).tolist() + [self.sequence_length]\n",
    "\n",
    "            for i in range(len(indexes) - 1):\n",
    "                i0, i1 = indexes[i], indexes[i + 1]\n",
    "                rnn_output, hidden_states = self.rnn(rnn_input[:,i0:i1,:], hidden_states)\n",
    "                hidden_states[:, (terminated[:,i1-1]), :] = 0\n",
    "                rnn_outputs.append(rnn_output)\n",
    "\n",
    "            rnn_output = torch.cat(rnn_outputs, dim=1)\n",
    "        # no need to reset the RNN state in the sequence\n",
    "        else:\n",
    "            rnn_output, hidden_states = self.rnn(rnn_input, hidden_states)\n",
    "\n",
    "        # flatten the RNN output\n",
    "        rnn_output = torch.flatten(rnn_output, start_dim=0, end_dim=1)  # (N, L, D ∗ Hout) -> (N * L, D ∗ Hout)\n",
    "\n",
    "        x = F.relu(self.linear_layer_1(torch.cat([rnn_output, inputs[\"taken_actions\"]], dim=1)))\n",
    "\n",
    "        return self.linear_layer_3(x), {\"rnn\": [hidden_states]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d19b1",
   "metadata": {},
   "source": [
    "### Parameters of the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca189da-c534-4e14-8498-06e98b21b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMUEnv(FMUGym):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.setpoint = None\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {'info_time':time.time(), \"setpoint\":self.setpoint}\n",
    "\n",
    "    # possibility to do processing of FMU outputs to observations of the environment, \n",
    "    # here the observation is composed of first two entries of FMU outputs and the following two as setpoints to these outputs\n",
    "    def _get_obs(self):\n",
    "        \n",
    "        self._get_fmu_output()\n",
    "\n",
    "        obs = np.array(list(self.observation.values())).flatten()\n",
    "        \n",
    "        noisy_observation = obs + self._get_output_noise()\n",
    "        \n",
    "        self.setpoint = self.setpoint_trajectory(self.y_start, self.y_stop, self.time)\n",
    "        full_obs = np.append(noisy_observation, self.setpoint)\n",
    "        \n",
    "        return full_obs\n",
    "    \n",
    "    def _get_input_noise(self):\n",
    "        input_noise = []\n",
    "        for inp_name in self.input_dict:\n",
    "            input_noise.append(self.input_noise[inp_name].sample()[0])\n",
    "        return np.array(input_noise)\n",
    "\n",
    "    def _get_output_noise(self):\n",
    "        output_noise = []\n",
    "        for out_name in self.output_dict:\n",
    "                output_noise.append(self.output_noise[out_name].sample()[0])\n",
    "        return np.array(output_noise)\n",
    "\n",
    "    def _get_terminated(self):\n",
    "        if self.time > self.stop_time:\n",
    "            self.reset()\n",
    "            return True, False\n",
    "\n",
    "        for termination in self.terminations:\n",
    "            min = self.terminations[termination].low[0]\n",
    "            max = self.terminations[termination].high[0]\n",
    "            if self.observation[termination] < min or self.observation[termination] > max:\n",
    "                self.reset()\n",
    "                return False, True\n",
    "\n",
    "        return False, False\n",
    "\n",
    "    def _create_action_space(self, inputs):\n",
    "        lows = []\n",
    "        highs = []\n",
    "        for inp in inputs:\n",
    "            lows.append(inputs[inp].low[0])\n",
    "            highs.append(inputs[inp].high[0])\n",
    "        action_space = gym.spaces.Box(low=np.array(lows), high=np.array(highs), dtype=np.float32)\n",
    "        return action_space\n",
    "    \n",
    "    def _create_observation_space(self, outputs):\n",
    "        lows = []\n",
    "        highs = []\n",
    "        for out in outputs:\n",
    "            lows.append(outputs[out].low[0])\n",
    "            highs.append(outputs[out].high[0])\n",
    "        observation_space = gym.spaces.Box(low=np.array(lows), high=np.array(highs), dtype=np.float32)\n",
    "        return observation_space\n",
    "\n",
    "    def _noisy_init(self):\n",
    "        # add noise to setpoint goals\n",
    "        for ye in self.y_stop:\n",
    "            self.y_stop[ye] = self.y_stop_range[ye].sample()[0]\n",
    "        \n",
    "        # add noise to initial system state\n",
    "        init_states = {}\n",
    "        for var in self.random_vars_refs:\n",
    "            var_ref = self.random_vars_refs[var][0]\n",
    "            uniform_value = self.random_vars_refs[var][1].sample()[0]\n",
    "            init_states[var_ref] = uniform_value\n",
    "\n",
    "            # domain randomization with noisy initial y_start\n",
    "            if var in self.rand_starts.keys():\n",
    "                input_string = self.rand_starts[var]\n",
    "                self.y_start[input_string] = float(uniform_value)\n",
    "        \n",
    "        return init_states\n",
    "\n",
    "    def _process_action(self, action):\n",
    "        processed_action = action + self._get_input_noise()\n",
    "        return processed_action\n",
    "\n",
    "    def setpoint_trajectory(self, y_start, y_stop, time):\n",
    "        y = []\n",
    "        for y0, ye in zip(y_start.values(), y_stop.values()):\n",
    "            y.append((ye - y0)/(self.stop_time-self.start_time)*(time-self.start_time) + y0)\n",
    "        return np.array(y)\n",
    "\n",
    "    def _process_reward(self, obs, acts, info):\n",
    "        reward = self.compute_reward(obs, info)\n",
    "        return reward\n",
    "    \n",
    "    def compute_reward(self, obs, info):\n",
    "        # Deceptive reward: it is positive (0) only when the goal is achieved\n",
    "        # Here we are using a vectorized version\n",
    "        control_error = obs[:2] - obs[2:]\n",
    "        reward = - np.sum(control_error**2)\n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e20963-cfc0-4e45-8d6e-f8df2f1c3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = VarSpace(\"inputs\")\n",
    "inputs.add_var_box(\"input1\", -2.0, 2.0)\n",
    "inputs.add_var_box(\"input2\", -2.0, 2.0)\n",
    "\n",
    "input_noise = VarSpace(\"input_noise\")\n",
    "input_noise.add_var_box(\"input1\", 0.0, 0.0)\n",
    "input_noise.add_var_box(\"input2\", 0.0, 0.0)\n",
    "\n",
    "outputs = VarSpace(\"outputs\")\n",
    "outputs.add_var_box(\"output1\", -1e6, 1e6)\n",
    "outputs.add_var_box(\"output2\", -1e6, 1e6)\n",
    "outputs.add_var_box(\"setpoint1\", -1e6, 1e6)\n",
    "outputs.add_var_box(\"setpoint2\", -1e6, 1e6)\n",
    "\n",
    "output_noise = VarSpace(\"output_noise\")\n",
    "output_noise.add_var_box(\"output1\", 0.0, 0.0)\n",
    "output_noise.add_var_box(\"output2\", 0.0, 0.0)\n",
    "\n",
    "# dynamics and domain randomization\n",
    "random_vars = VarSpace(\"random_vars\")\n",
    "random_vars.add_var_box(\"firstOrder.k\", 4, 6)\n",
    "random_vars.add_var_box(\"firstOrder1.k\", 4, 6)\n",
    "random_vars.add_var_box(\"firstOrder.y_start\", -0.5, 0.5)\n",
    "random_vars.add_var_box(\"firstOrder1.y_start\", -0.5, 0.5)\n",
    "\n",
    "# map state variables to corresponding outputs of Modelica Model\n",
    "set_point_map = State2Out(\"set_point_map\")\n",
    "set_point_map.add_map(\"firstOrder.y_start\", \"output1\")\n",
    "set_point_map.add_map(\"firstOrder1.y_start\", \"output2\")\n",
    "\n",
    "set_point_nominal_start = TargetValue(\"set_point_nominal_start\")\n",
    "set_point_nominal_start.add_target(\"output1\", 0.0)\n",
    "set_point_nominal_start.add_target(\"output2\", 0.0)\n",
    "\n",
    "set_point_stop = VarSpace(\"set_point_stop\")\n",
    "set_point_stop.add_var_box(\"output1\", 1.0, 2.5)\n",
    "set_point_stop.add_var_box(\"output2\", 1.2, 3.0)\n",
    "\n",
    "terminations = VarSpace(\"terminations\")\n",
    "\n",
    "config = FMUGymConfig(fmu_path=os.path.abspath('FMUs/dummy_for_FMU_cosim.fmu'),\n",
    "                      start_time=0.0,\n",
    "                      stop_time=10.0,\n",
    "                      sim_step_size=0.01,\n",
    "                      action_step_size=0.01,\n",
    "                      inputs=inputs,\n",
    "                      input_noise=input_noise,\n",
    "                      outputs=outputs,\n",
    "                      output_noise=output_noise,\n",
    "                      random_vars=random_vars,\n",
    "                      set_point_map=set_point_map,\n",
    "                      set_point_nominal_start=set_point_nominal_start,\n",
    "                      set_point_stop=set_point_stop,\n",
    "                      terminations=terminations\n",
    "                     )                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82741f5-2bd4-45dc-966e-3083f2cb13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_description = read_model_description(config.fmu_path)\n",
    "print(model_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe83cf",
   "metadata": {},
   "source": [
    "### Creation of gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9baf88f-91cb-481b-9e96-0c6808ca36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummyEnv = FMUEnv(config)\n",
    "env = wrap_env(dummyEnv, wrapper=\"gymnasium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414faf54-65b5-4164-84f2-28b6bf0a9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = env.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403e709-0a6e-4568-93f6-af3ec86d3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a memory as experience replay\n",
    "memory = RandomMemory(memory_size=2000, num_envs=env.num_envs, device=device, replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8b5b7e-7289-4f8f-9bd8-2dc53b216c59",
   "metadata": {},
   "source": [
    "### Creation of SKRL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444af55a-ef59-4290-a1aa-56300cdfc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the agent's models (function approximators).\n",
    "# SAC requires 5 models, visit its documentation for more details\n",
    "# https://skrl.readthedocs.io/en/latest/api/agents/sac.html#models\n",
    "models = {}\n",
    "models[\"policy\"] = Actor(env.observation_space, env.action_space, device, clip_actions=True, num_envs=env.num_envs)\n",
    "models[\"critic_1\"] = Critic(env.observation_space, env.action_space, device, num_envs=env.num_envs)\n",
    "models[\"critic_2\"] = Critic(env.observation_space, env.action_space, device, num_envs=env.num_envs)\n",
    "models[\"target_critic_1\"] = Critic(env.observation_space, env.action_space, device, num_envs=env.num_envs)\n",
    "models[\"target_critic_2\"] = Critic(env.observation_space, env.action_space, device, num_envs=env.num_envs)\n",
    "\n",
    "# initialize models' parameters (weights and biases)\n",
    "for model in models.values():\n",
    "    model.init_parameters(method_name=\"normal_\", mean=0.0, std=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96084cfd-1e41-45dd-82ea-987821940fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure and instantiate the agent (visit its documentation to see all the options)\n",
    "# https://skrl.readthedocs.io/en/latest/api/agents/sac.html#configuration-and-hyperparameters\n",
    "cfg = SAC_DEFAULT_CONFIG.copy()\n",
    "cfg[\"discount_factor\"] = 0.99\n",
    "cfg[\"batch_size\"] = 64\n",
    "cfg[\"random_timesteps\"] = 0\n",
    "cfg[\"learning_starts\"] = 100\n",
    "cfg[\"learn_entropy\"] = True\n",
    "# logging to TensorBoard and write checkpoints (in timesteps)\n",
    "cfg[\"experiment\"][\"write_interval\"] = 500\n",
    "cfg[\"experiment\"][\"checkpoint_interval\"] = 2000\n",
    "cfg[\"experiment\"][\"directory\"] = \"runs/torch/FMUGym\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580543a1-24c9-44e9-9f94-ae00503bb790",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SAC(models=models,\n",
    "            memory=memory,\n",
    "            cfg=cfg,\n",
    "            observation_space=env.observation_space,\n",
    "            action_space=env.action_space,\n",
    "            device=device)\n",
    "\n",
    "\n",
    "# configure and instantiate the RL trainer\n",
    "cfg_trainer = {\"timesteps\": int(1e5), \"headless\": True}\n",
    "trainer = SequentialTrainer(cfg=cfg_trainer, env=env, agents=[agent])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143def4d",
   "metadata": {},
   "source": [
    "### Learning of SKRL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f9181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timer_start_time = time.time()\n",
    "trainer.train()\n",
    "print(\"learn duration: \", time.time() - timer_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4852fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"trained_agents/skrl_sac.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a95826",
   "metadata": {},
   "source": [
    "### ... or loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ab799-fe23-428d-863a-0239dd08ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load(\"trained_agents/skrl_sac.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615fb30",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c66bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "obs_array = []\n",
    "act_array = []\n",
    "obs_array.append(observation.numpy()[0][0:4])\n",
    "rew_sum = 0\n",
    "\n",
    "num_steps = int((config.stop_time - config.start_time) / config.sim_step_size)\n",
    "for num_step in range(num_steps):\n",
    "    action, _, _ = agent.act(states=observation, timestep=num_step, timesteps=num_steps)\n",
    "    action = action.clone().detach().requires_grad_(False) # transform it to be used in env\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    rew_sum += reward\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "    obs_array.append(observation.numpy()[0][:4])\n",
    "    act_array.append(action.numpy())\n",
    "\n",
    "obs_array = np.array(obs_array).flatten()\n",
    "obs_array = obs_array.reshape(-1, 4)  \n",
    "act_array = np.array(act_array).flatten()\n",
    "act_array = act_array.reshape(-1, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf9fc63-6c20-4dd1-b8f6-02f4ceb3c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_setpoints = obs_array[:, 2]\n",
    "y2_setpoints = obs_array[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(obs_array[:, 0], label='y1', color=\"b\")\n",
    "plt.plot(y1_setpoints, label='y1_set', color=\"b\", linestyle='--')\n",
    "plt.plot(obs_array[:, 1], label='y2', color=\"r\")\n",
    "plt.plot(y2_setpoints, label='y2_set', color=\"r\", linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(act_array[:, 0], label='u1', color=\"b\")\n",
    "plt.plot(act_array[:, 1], label='u2', color=\"r\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4872f6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummyEnv.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
